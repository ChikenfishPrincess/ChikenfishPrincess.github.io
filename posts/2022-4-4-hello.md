# 深度学习笔记：02详解神经网络中的神经元和激活函数
- xor异或运算 -相同为0不同为1。
- 深度学习的神经网络借助了生物学对脑神经系统的研究成果。

## 1、生物学模型
![在这里插入图片描述](https://img-blog.csdnimg.cn/d886f7c931374c3eae63b3ad4b7ace3f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

dendrites突触接收外界输入的信号以后，中间的axon轴突会把dendrites接受到的信号进行整合处理，右边的terminals叫终端输出，他会把轴突整合后的信号再切割成多个部分分别输送给其它神经元。

### 神经元的工作机制：

神经元接受的是电信号然后输出另一种电信号，但是如果输入电信好的强度不够大，那么神经元就不会做出任何的反应，如果电信好的强度大于某个界限，那么神经元就会作出反应向其他神经元传递信号。

Q:那为什么生物大脑的运算能力远远逊色于计算机，但是生物却能轻而易举的实现控制飞行，寻找食物，识别等功能而计算机不可以呢？

**A:因为生物大脑的运算运行存在“模糊性”，而电子计算机不行。**
### 所以下面是计算机如何实现：“模糊性”：
可以通过模糊传递信号的来源处理，神经网络算法中设计的神经元会同时接收多个输入参数，他把这些参数加总求和，然后带入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中有某个值很大其他的都很小那么加总后值很大， 输出的信号值就会变大，如果每个参数都不算太大，但是加总后结果很大，于是输出的信号值就会很大。这种情况就使得运算具备一定的模糊性，这样就跟生物大脑的神经元运转方式很像。

并且神经元并不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应：
![在这里插入图片描述](https://img-blog.csdnimg.cn/0dc0abc775434f1bba1b2c8305e09cd6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center)


## 2、数学模型

![在这里插入图片描述](https://img-blog.csdnimg.cn/c3a25b62ada54a23a921e9c9129cbac1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center)


其中：x1,x2,x3模拟神经元沿着axon轴突通过dendrite突触、树突传送到此神经元的输入信号。
w1,w2表示该输入对神经元影响的大小
然后该神经元细胞通过对应的w1,w2参数相乘、求和，得到最终的输出。
f为激活函数（activation funtion）,b表示偏置项（bias）

## 3、激活函数（activation function）

输入的信号，在神经元体内相加，当超过某一个阈值时神经元就会起火（fire）,并沿着轴突产生一个尖峰（spike） 在数学模型中，我们不考虑尖峰产生的精确时间的影响，只考虑神经元起火的频率所带来的影响，为了模拟起火频率，设计出激活函数来表示沿着轴突传递尖峰的频率。

### 常见的激活函数有以下几种:
#### （1）step:步调函数-----完全淘汰
```python
import matplotlib.pyplot as plt
x = [1,2,3,4]
y = [0,1,2,3]
plt.step(x,y)#步调函数---产生这个图像
plt.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/f5470e5f4fa5495db19da7ab7cfe5ae3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center)


但是step:步调函数更倾向于理论而不是实际,它模仿了生物神经元要么全有要么全无的属性。它无法应用于神经网络,因为其导数是 0(除了零点导数无定义以外),这意味着基于梯度的优化方法并不可行。

#### （2）Sigmoid:sigmoid函数-----以前常用
是非线性的，其数学公式为σ(x)=1/(1+ex)
```python
from matplotlib import pylab
import pylab as plt
import numpy as np
def sigmoid(x):
    return (1/(1+np.exp(-x)))
mySamples = []
mySigmoid = []
x = plt.linspace(-10,10,10)
y = plt.linspace(-10,10,100)
plt.plot(x,sigmoid(x),'r',label = 'linspace(-10,10,10)')
plt.plot(y,sigmoid(y),'r',label = 'linspace(-10,10,1000)')
plt.grid()

plt.title('Sigmoid function')
plt.suptitle('Sigmoid')

plt.legend(loc = 'lower right')


plt.text(4,0.8,r'$\sigma(x)=\frac{1}{1+e^(-x)}$',fontsize = 15)

plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))
plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))

plt.xlabel('X Axis')
plt.ylabel('Y Axis')

plt.show()
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/a9fc1da2ae52485585c1922ffa9be685.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center)


数值映射到0-1范围内，越小的数越趋近于0，越大的数越趋近于1。

但是现在Sigmoid函数也已经过时了，人们也很少会使用它,因为他有两个严重缺陷缺陷：

1. 容易饱和杀死梯度。该激活函数在0或者1的状态下会饱和，梯度在这些地方接近于0。那么在反向传播的过程中由于梯度太小，而导致神经元相应的权重参数几乎无法更新，并且也会影响后续参数的更新。由于函数在饱和状态下，会产生杀死梯度的现象，那么我们在初始化参数时，就更加注意来防止饱和现象的发生。
2. Sigmoid函数的输出并不是以0对称的。那么在梯度反向传播的过程中要么是全正，要么是全负（根据表达式f = wTx+b来决定），这样就会带来参数更新过程中不理想的锯齿形变化。当然这点的严重程度比上点低得多。

#### （3）ReLU函数 -- 函数公式：f(x) = max(0,x)

```python

import numpy as np
import matplotlib.pyplot as plt
 
 
def relu(x):
    return np.maximum(0, x)
 
 
relu_inputs = np.arange(-10, 10, 0.1)
relu_outputs = relu(relu_inputs)
print("Relu Function Input :: \n{}".format(relu_inputs))
print("Relu Function Output :: \n{}".format(relu_inputs))
 
plt.plot(relu_inputs, relu_outputs)
plt.xlabel("Relu Inputs")
plt.ylabel("Relu Outputs")

```

![在这里插入图片描述](https://img-blog.csdnimg.cn/4ae5a55f0bf84c3c88862d64895a9673.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWHUyOTU0MTk1Nzg=,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center)

相比Sigmod和tanh有以下几个优点：
1. 克服梯度消失问题
2. 加快训练速度。----正是由于克服了梯度消失问题所以才可以加快训练速度。

缺点：
1. 输入负数，则完全不激活，ReLU函数死掉。
2. ReLU函数要么是0，要么是正数，所以ReLU函数是不以0为中心的函数。
   
## 4、小结
深度学习中最大的问题是梯度消失问题，使用tanh、sigmod等饱和激活函数情况下特别严重（神经网络在进行方向误差传播时，各个层都要乘以激活函数的一阶导数，梯度每传递一层就会衰减一层，网络层数较多时，梯度G就会不停衰减直到消失），使得训练网络收敛越来越慢，而ReLU函数凭借其线性、非饱和的形式，训练速度则快很多。